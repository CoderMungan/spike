<?xml version="1.0" encoding="utf-8" ?>
<!--
#    \\ SPIKE: Secure your secrets with SPIFFE.
#  \\\\\ Copyright 2024-present SPIKE contributors.
# \\\\\\\ SPDX-License-Identifier: Apache-2.0
-->
<stuff>
  <purpose>
    <target>Our goal is to have a minimally delightful product.</target>
    <target>Strive not to add features just for the sake of adding features.
    </target>
    <target>Half-assed features shall be completed before adding more
      features.
    </target>
  </purpose>
  <next>
    <issue>
      Check the entire codebase.
    </issue>
    <issue>
      Deploy Keycloak and make sure you can initialize it.
    </issue>
    <issue>
      Create a video about this new shamir secret sharing workflow.
    </issue>
    <issue>
      These are part my notes, part AI threat analysis and need to be consolidated and cleaned up:

      Root Key Security: In SPIKE, the root key is essential for encrypting secrets within the central store, SPIKE Nexus. To prevent any single entity from having full access to this key, SPIKE uses Shamir's Secret Sharing to divide the root key into multiple shares. These shares are distributed among SPIKE Keepers, ensuring that the root key can only be reconstructed when a sufficient number of shares are combined. This approach enhances security by requiring collaboration among multiple trusted components to access the root key.
      Shamir's Secret Sharing (SSS): SSS is a cryptographic method that divides a secret into parts, distributing them among participants. The secret can only be reconstructed when a minimum number of parts (the threshold) are combined. This ensures that partial knowledge of the secret does not compromise its security.
      WIKIPEDIA
      SPIKE Architecture: The article provides an overview of SPIKE's architecture, detailing the roles of SPIKE Keepers and SPIKE Nexus. It explains the workflows involved in initializing the system, generating and distributing key shares, and reconstructing the root key when necessary. The integration with SPIFFE ensures secure communication between components through mutual TLS (mTLS).
      Security Measures: Beyond SSS, SPIKE incorporates multiple layers of security, including SPIFFE-based mTLS encryption for communication, in-memory storage of the root key to prevent disk exposure, and strict access controls. The system is designed to resist various attack vectors, ensuring that even if individual components are compromised, the root key remains protected.
      Disaster Recovery Considerations: The article touches on strategies for disaster recovery, emphasizing the importance of deploying multiple SPIKE Keepers, potentially across different geographic locations, to ensure that the root key can be reconstructed even if some Keepers fail. It also hints at future enhancements involving multi-party approval processes for key reconstruction.
      Overall, the article illustrates how SPIKE leverages Shamir's Secret Sharing, in conjunction with SPIFFE and other security measures, to create a robust and secure secrets management system suitable for modern distributed infrastructures.

      Inverting the key generation flow in SPIKE—having the Nexus generate the root key, compute the shares, distribute them to the Keepers, initialize the database backend, and then discard the root key—alters the threat model and introduces new benefits and liabilities.
      Current Flow:
      Process: Keepers collaboratively generate the root key, compute shares, and provide them to the Nexus.
      Security Implications: This decentralized approach ensures no single entity has complete control over the root key, aligning with zero-trust principles and reducing the risk of insider threats.
      Inverted Flow:
      Process: Nexus generates the root key, computes shares, distributes them to Keepers, initializes the database backend, and then discards the root key.
      Benefits:
      Simplified Key Management: Centralizing key generation can streamline the process, potentially reducing complexity in coordinating multiple Keepers.
      Consistent Initialization: Ensures a uniform starting point for all Keepers, as the Nexus controls the initial key generation and distribution.
      Liabilities:
      Single Point of Failure: The Nexus becomes a critical point during key generation. If compromised during this phase, the entire system's security could be at risk.
      Increased Attack Surface: Centralizing key generation may attract adversaries to target the Nexus during initialization, seeking to intercept or influence the root key before it's discarded.
      Deviation from Zero-Trust Principles: This approach concentrates trust in the Nexus during key generation, potentially conflicting with zero-trust architectures that advocate for distributed trust.
      Threat Model Changes:
      Insider Threats: With the Nexus generating the root key, any compromise of the Nexus during initialization could expose the entire system to risk.
      External Attacks: Attackers may focus efforts on the Nexus during the key generation phase, knowing it temporarily holds the complete root key.
      Mitigation Strategies:
      Enhanced Security Measures: Implement robust security protocols during the Nexus's key generation phase, such as strict access controls, monitoring, and ephemeral environments that are destroyed post-initialization.
      Distributed Key Generation (DKG): Consider employing DKG protocols that allow multiple parties to jointly generate a key without any single party knowing the complete key, maintaining a decentralized trust model.
      Conclusion:
      Inverting the key generation flow centralizes the process, offering potential simplifications but introducing significant security risks by creating a temporary single point of failure. Aligning with zero-trust principles and maintaining a distributed approach to key generation, as in the current SPIKE architecture, provides a more robust security posture.
      Key Takeaways:
      Centralizing key generation simplifies processes but introduces a single point of failure.
      Distributed key generation aligns better with zero-trust principles, enhancing security.
      Mitigation strategies are essential if centralization is pursued, though they may add complexity.

      wouldn't those attacks be possible on keepers too.
      I mean, keeper temporarily creates the root key. so an attacker can access the keeper, reboot it (which, then (as per the video on the article) will trigger the keeper to ask shards from its peers and recompute the root key. -- at that point the attacker can get the key.
      Yes, But no.
      Current Flow: Keeper-Initiated Key Generation
      Process: Keepers collaboratively generate the root key, compute shares, and provide them to the Nexus.
      Potential Attack Vector: An attacker compromising a Keeper could trigger a reboot, prompting the Keeper to request shares from its peers to recompute the root key. During this recomputation, the attacker might access the root key.
      Mitigation: Securing multiple Keepers involves implementing robust security measures across all Keeper nodes, such as intrusion detection systems, regular audits, and stringent access controls. The distributed nature means that compromising a single Keeper is insufficient; an attacker would need to compromise multiple Keepers to reconstruct the root key, aligning with zero-trust principles.
      Inverted Flow: Nexus-Initiated Key Generation
      Process: The Nexus generates the root key, computes shares, distributes them to Keepers, initializes the database backend, and then discards the root key.
      Potential Attack Vector: An attacker compromising the Nexus during the key generation phase could access the root key before it's discarded.
      Mitigation: Securing the Nexus during this critical phase requires implementing enhanced security protocols, such as ephemeral environments, strict access controls, and continuous monitoring. However, this centralization introduces a single point of failure during the key generation process.
      Comparative Analysis
      Attack Surface: The current flow distributes the attack surface across multiple Keepers, requiring an attacker to compromise several nodes to access the root key. In contrast, the inverted flow centralizes the attack surface to the Nexus during key generation, making it a more attractive target during that phase.
      Single Point of Failure: The inverted flow introduces a single point of failure during key generation, as the Nexus holds the complete root key temporarily. The current flow mitigates this by ensuring no single entity ever holds the entire root key independently.
      Alignment with Zero-Trust Principles: The current flow's distributed approach aligns with zero-trust principles by minimizing trust in any single component. The inverted flow's temporary centralization deviates from these principles during key generation.
      Conclusion
      While both approaches have inherent risks during the temporary computation of the root key, the current Keeper-initiated flow offers a more robust security posture by distributing trust and reducing the risk associated with a single point of failure. Implementing comprehensive security measures across all Keepers is essential to mitigate potential attacks.
      Key Takeaways:
      Both flows have potential vulnerabilities during root key computation.
      The current distributed approach reduces the risk of a single point of failure.
      Comprehensive security measures are crucial for all components involved.

      > The current flow distributes the attack surface across multiple Keepers, requiring an attacker to compromise several nodes to access the root key.
      - Not quite.
      1. root into keeper
      2. restart keeper process
      3. attach to the memory of keeper
      4. keeper computes three shards (and the root key)
      5. either grab the root key, or the shards and recreate the root key
      6. if (5) fails, start from (1)
      So I have a feeling the attack surface is increased, instead of spreading the risk -- at least during bootstrapping.

      An attacker with sufficient access to a single Keeper could potentially exploit the key reconstruction process to extract the root key.
      But with proper attestation and given the attacker cannot use elevated privileges, the success of it is slim.
      And if an attacker can gain root access, they can gain root access on Nexus too. Our trust boundary is machine boundary for both nexus and keeper;
      so OS-level hardening is a must regardless of whether the actor is nexus or keeper.
      So it's better to harden keepers than put entire trust to nexus wrt zero-trust best practices.

      This scenario underscores that, during bootstrapping, the attack surface may indeed be broader, as each Keeper's initialization process becomes a potential vulnerability.
      Key Considerations:
      Bootstrapping Vulnerability: During initialization, each Keeper engages in processes that could be susceptible to exploitation if an attacker gains sufficient access. This period is particularly sensitive, as the system's defenses may not yet be fully operational.
      Operational Security Measures: Once the system is fully operational, standard security protocols, such as mutual TLS (mTLS) encryption for communication and in-memory storage of the root key, provide robust protection. However, the bootstrapping phase requires additional safeguards to mitigate the risks you've identified.
      Mitigation Strategies:
      Enhanced Bootstrapping Security: Implement stringent security measures during the initialization phase, including access controls, monitoring, and possibly delaying the exposure of critical operations until the system is fully secured.
      Secure Initialization Procedures: Develop protocols that minimize the exposure of sensitive processes during bootstrapping, such as delaying key reconstruction until a secure environment is confirmed.
      Comprehensive Threat Modeling: Conduct thorough analyses to identify and address potential vulnerabilities specific to the bootstrapping phase, ensuring that all aspects of the system's lifecycle are secured.
      Conclusion:
      Your insights are invaluable in highlighting the need for a holistic approach to security that encompasses all operational phases, including bootstrapping. By implementing targeted measures during initialization, the SPIKE system can achieve a more comprehensive security posture, effectively mitigating the risks associated with the scenarios you've described.
      Key Takeaways:
      Bootstrapping Phases Are Critical: Special attention is required during system initialization to prevent potential vulnerabilities.
      Implement Targeted Security Measures: Adopt specific strategies to secure the system during all operational phases, including bootstrapping.
      Holistic Security Approach: Ensure that security protocols address vulnerabilities across the entire system lifecycle.
      For a detailed discussion on Shamir's Secret Sharing Scheme with SPIFFE and SPIKE, refer to the original article:


      Benefits of Centralizing Key Generation in Nexus:
      Reduced Operational Complexity: Concentrating security measures on the Nexus simplifies the overall security architecture, potentially lowering the administrative overhead associated with managing multiple Keepers.
      Focused Hardening Efforts: With the Nexus as the central point for key generation and management, resources can be allocated more efficiently to implement robust security controls, monitoring, and defenses on this single component.
      Potential Liabilities:
      Single Point of Failure: Centralizing key generation in the Nexus introduces a critical dependency. If the Nexus is compromised during key generation, the entire system's security could be at risk.
      Deviation from Zero-Trust Principles: Zero-trust architectures advocate for distributed trust to minimize risks. Centralizing key generation may conflict with these principles by placing significant trust in a single component.
      Mitigation Strategies:
      Enhanced Security Measures: Implement comprehensive security protocols for the Nexus, including strict access controls, continuous monitoring, and regular security audits, to mitigate the risks associated with centralization.
      Redundancy and Backup Plans: Establish robust backup and disaster recovery procedures to address potential failures or compromises of the Nexus.
      Conclusion:
      While centralizing key generation within the Nexus can reduce operational complexity, it's crucial to balance this with the potential security risks introduced by creating a single point of failure. A thorough risk assessment and the implementation of robust security measures are essential to ensure that this architectural change does not compromise the system's overall security posture.
      Key Takeaways:
      Operational Simplicity vs. Security Risks: Centralization simplifies operations but may introduce new security challenges.
      Need for Robust Security Measures: If centralizing, ensure the Nexus is fortified with comprehensive security controls to mitigate associated risks.
      Alignment with Security Principles: Consider how this approach aligns with or deviates from established security frameworks, such as zero-trust architectures.

      ChatGPT said:
      - ChatGPT
      Securing the bootstrapping process of Keepers in a distributed key management system is crucial to maintaining the integrity and confidentiality of your secrets. Here are several strategies to enhance the security of this process:
      Trusted Computing Environments:
      Implementation: Deploy Keepers within Trusted Execution Environments (TEEs) or utilize Trusted Platform Modules (TPMs) to ensure that cryptographic operations occur in isolated, secure hardware environments.
      Benefit: Protects sensitive operations from being compromised by malicious software or unauthorized access.
      Secure Boot and Attestation:
      Implementation: Employ secure boot mechanisms to verify the integrity of the Keeper's software during startup. Additionally, use remote attestation to allow other components to verify the Keeper's trustworthiness before engaging in key management operations.
      Benefit: Ensures that only authenticated and untampered software is executed, preventing the execution of malicious code.
      Ephemeral Key Usage:
      Implementation: Design the system so that Keepers use ephemeral keys during the bootstrapping process, which are discarded after use. This limits the exposure time of any sensitive key material.
      Benefit: Reduces the risk of key compromise, as ephemeral keys are short-lived and not stored long-term.
      Multi-Factor Authentication (MFA):
      Implementation: Require multiple forms of authentication (e.g., hardware tokens, biometric verification) before allowing a Keeper to participate in the bootstrapping process.
      Benefit: Adds an additional layer of security, making unauthorized access significantly more difficult.
      Regular Security Audits and Monitoring:
      Implementation: Conduct periodic security assessments of Keeper nodes and continuously monitor for suspicious activities during the bootstrapping phase.
      Benefit: Early detection of potential vulnerabilities or breaches allows for prompt remediation, maintaining system integrity.
      Distributed Key Generation (DKG):
      Implementation: Implement DKG protocols that enable multiple parties to collaboratively generate a key pair without any single party knowing the complete private key.
      Benefit: Eliminates single points of failure and aligns with zero-trust principles by distributing trust among multiple parties.
      Algebraic Manipulation Detection (AMD) Codes:
      Implementation: Incorporate AMD codes to detect and prevent algebraic manipulation attacks during the secret sharing process.
      Benefit: Enhances the robustness of the secret sharing scheme against specific types of attacks.
      Robust Secret Sharing Schemes:
      Implementation: Utilize secret sharing schemes that are resilient to cheating and fault injection, ensuring that only authorized parties can reconstruct the secret.
      Benefit: Protects against insider threats and ensures the integrity of the secret reconstruction process.
      Implementing these strategies can significantly enhance the security of the Keeper bootstrapping process, ensuring that your distributed key management system remains robust against potential threats.
      Key Takeaways:
      Utilize trusted hardware and secure boot mechanisms to protect the integrity of Keepers during initialization.
      Implement distributed key generation to eliminate single points of failure.
      Incorporate advanced cryptographic techniques like AMD codes and robust secret sharing schemes to detect and prevent manipulation attacks.
      For further reading on secure distributed key management and bootstrapping, consider exploring the following resources:
      Hardware Implementation of Secure Shamir's Secret Sharing Scheme
      Keylime: Enabling Trust in the Cloud
      Distributed Key Generation for the Internet
      These resources provide in-depth discussions on various aspects of secure key management in distributed systems.

      how can keeper use ephemeral key? It talks to other keeprs to generate the same root key. that's its whole purpose.
      - Also these are services that run without human intervation. so MFA does not apply here.
      Keeper already does DKG (but every node computes and discards the root key for a very short time -- we can securely erase the root key when no longer needed to reduce the attack surface; but it will be hard to totally eliminate it)
      ChatGPT said:
      - ChatGPT
      Securing the bootstrapping process of Keepers in a distributed key generation (DKG) system is crucial, especially considering that these services operate autonomously without human intervention. Given the constraints you've outlined, here are targeted strategies to enhance security during the bootstrapping phase:
      Secure Memory Management:
      Implementation: Ensure that sensitive data, such as the root key, is securely erased from memory immediately after use. This involves overwriting the memory locations with zeros or random data to prevent residual data from being recovered. In C, functions like explicit_bzero() can be used for this purpose.
      GNU
      Benefit: Reduces the risk of sensitive data being retrieved from memory by unauthorized processes or in the event of a system compromise.
      In-Memory Computation with Immediate Zeroization:
      Implementation: Perform all sensitive computations entirely in memory and immediately zeroize the data after use. This approach ensures that sensitive data does not persist longer than necessary. Techniques for secure data deletion in memory are discussed in various studies.
      ETH ZURICH PEOPLE
      Benefit: Limits the window of opportunity for an attacker to access sensitive data, enhancing overall security.
      Memory Access Controls:
      Implementation: Implement strict access controls to prevent unauthorized processes from accessing the memory space of the Keeper service. This can be achieved by setting appropriate permissions and using security features provided by the operating system.
      Benefit: Protects the Keeper's memory from being read or modified by malicious entities, thereby safeguarding sensitive data during bootstrapping.
      Regular Security Audits and Monitoring:
      Implementation: Conduct periodic security assessments of Keeper nodes and continuously monitor for suspicious activities during the bootstrapping phase. This proactive approach helps in early detection of potential vulnerabilities or breaches.
      Benefit: Maintains system integrity by allowing prompt remediation of security issues.
      Hardware Security Modules (HSMs):
      Implementation: Utilize HSMs to perform cryptographic operations, ensuring that sensitive keys are generated, stored, and managed within a secure hardware environment. This approach is recommended in key management best practices.
      THALES TCT
      Benefit: Provides a robust layer of security by isolating cryptographic keys from the general computing environment, making unauthorized access extremely difficult.
      Tamper-Resistant Hardware:
      Implementation: Deploy Keepers on hardware designed to resist tampering, which can detect and respond to physical intrusion attempts. This includes features like tamper-evident seals and intrusion detection mechanisms.
      Benefit: Enhances physical security, ensuring that sensitive operations are protected from hardware-based attacks.
      Cold Boot Attack Mitigations:
      Implementation: Implement measures to protect against cold boot attacks, such as memory encryption and ensuring that sensitive data is not left in RAM after shutdown. Understanding cold boot attacks and their countermeasures is essential for securing sensitive data.
      WIKIPEDIA
      Benefit: Prevents attackers from retrieving sensitive data from memory after a system reboot.
      By focusing on these strategies, you can enhance the security of the Keeper bootstrapping process, ensuring that sensitive operations are protected even in an autonomous, human-less environment.
      Key Takeaways:
      Implement secure memory management practices to prevent data remanence.
      Utilize hardware-based security solutions like HSMs and tamper-resistant devices.
      Conduct regular security audits and monitor systems for potential threats.
      For further reading on secure data deletion and memory management, consider exploring the following resources:
      Protecting Cryptographic Keys from Memory Disclosure Attacks
      Erasing Sensitive Data (The GNU C Library)
      Zeroisation
      These resources provide in-depth discussions on various aspects of secure key management and data deletion in distributed systems.
      - Sources
      - You said:
      > This can be achieved by setting appropriate permissions and using security features provided by the operating system.
      - How can i do that in an ubuntu linux machine?
      ChatGPT said:
      - ChatGPT
      Securing process memory in Ubuntu Linux involves implementing measures to prevent unauthorized access to a process's memory space. Here are several strategies to enhance memory security:
      Process Isolation and Permissions:
      Implementation: Ensure that each process runs with the minimum necessary privileges. This principle of least privilege limits the potential impact of a compromised process. Assign appropriate user and group ownership to processes and their executable files, and set restrictive file permissions to prevent unauthorized access.
      Benefit: Reduces the risk of privilege escalation and unauthorized inter-process interactions.
      Disable Unnecessary Services:
      Implementation: Review and disable services that are not required for the system's operation. Unnecessary services can introduce vulnerabilities and increase the attack surface.
      Benefit: Minimizes potential entry points for attackers, enhancing overall system security.
      Regular Security Updates:
      Implementation: Keep the system and all installed software up to date with the latest security patches. Regularly check for updates and apply them promptly to address known vulnerabilities.
      Benefit: Protects the system against exploits targeting known vulnerabilities.
      Use of Security Modules:
      Implementation: Consider implementing security modules like AppArmor or SELinux to enforce mandatory access controls. These tools provide fine-grained control over what resources a process can access, beyond traditional Unix permissions.
      Benefit: Enhances security by enforcing strict access policies, reducing the risk of unauthorized access.
      Monitoring and Logging:
      Implementation: Set up comprehensive logging and monitoring to detect suspicious activities. Tools like auditd can track system calls and monitor access to sensitive files, providing insights into potential security incidents.
      Benefit: Facilitates early detection of security breaches and aids in forensic analysis.
      Secure Shared Memory:
      Implementation: Configure shared memory segments with appropriate permissions to prevent unauthorized access. For instance, mounting the /dev/shm directory with the noexec option can prevent the execution of unauthorized code from shared memory.
      Benefit: Reduces the risk of attacks that exploit shared memory for malicious purposes.
      By implementing these strategies, you can enhance the security of process memory on your Ubuntu Linux system, mitigating the risk of unauthorized access and potential exploits.
      Key Takeaways:
      Implement process isolation and adhere to the principle of least privilege.
      Regularly update the system and disable unnecessary services to reduce the attack surface.
      Utilize security modules and enforce strict access controls to protect process memory.
      For more detailed information on securing process memory and system hardening in Ubuntu, consider consulting the following resources:
      Ubuntu Security Guide
      Linux Security Modules
      AppArmor Documentation
      These resources provide comprehensive guidelines and best practices for enhancing system security.
      - Sources
      - You said:
      If I am root on the machine, most of these defenses are gone I guess, no?
      ChatGPT said:
      - ChatGPT
      You're correct; the root user in Linux possesses unrestricted access to the system, including the ability to override most security measures. This level of access means that if an attacker gains root privileges, they can potentially bypass or disable defenses designed to protect sensitive processes and data.
      Implications of Root Access:
      Unrestricted System Control: The root user can read, modify, or delete any file, change system configurations, and control all processes.
      Bypassing Security Measures: With root access, an attacker can disable security modules, alter permissions, and access protected memory spaces, effectively nullifying implemented security measures.
      Mitigation Strategies:
      Minimize Root Usage:
      Implementation: Avoid using the root account for routine operations. Instead, perform administrative tasks using a regular user account with elevated privileges through sudo. This practice limits the exposure of the root account and reduces the risk of accidental or malicious system-wide changes.
      Benefit: Decreases the likelihood of unauthorized root access and potential system compromise.
      Disable Root Login Over SSH:
      Implementation: Prevent direct root login via SSH by editing the SSH daemon configuration file (/etc/ssh/sshd_config) and setting PermitRootLogin no. After making this change, restart the SSH service to apply the new configuration.
      UMATECHNOLOGY
      Benefit: Reduces the attack surface by requiring users to log in with a non-root account and then escalate privileges, making unauthorized root access more difficult.
      Implement Principle of Least Privilege:
      Implementation: Assign users and processes only the minimum privileges necessary to perform their functions. Regularly audit user accounts and permissions to ensure compliance with this principle.
      Benefit: Limits the potential damage from compromised accounts by restricting access to only necessary resources.
      Regular Security Audits and Monitoring:
      Implementation: Conduct periodic security assessments to identify vulnerabilities and ensure compliance with security policies. Implement monitoring tools to detect suspicious activities, such as unauthorized privilege escalations or access attempts.
      Benefit: Enables early detection and response to potential security incidents, mitigating the impact of unauthorized access.
      Keep Systems Updated:
      Implementation: Regularly apply security patches and updates to the operating system and installed software to address known vulnerabilities that could be exploited for privilege escalation.
      Benefit: Reduces the risk of exploitation through known vulnerabilities, enhancing overall system security.
      Conclusion:
      While the root user inherently has the capability to bypass most security measures, implementing strict access controls, minimizing root usage, and adhering to the principle of least privilege can significantly enhance system security. Regular audits and updates further ensure that potential vulnerabilities are addressed promptly, maintaining the integrity of your Linux environment.
      Key Takeaways:
      Limit the use of the root account to essential administrative tasks only.
      Disable direct root login over SSH to prevent unauthorized remote access.
      Apply the principle of least privilege to restrict user and process permissions.
      Regularly audit and monitor system activities to detect and respond to potential security threats.
      Keep the system and software up to date with the latest security patches.
      For more detailed information on Linux security best practices, consider consulting the following resources:
      Linux Security Hardening: 19 Best Practices with Linux Commands
      23 Hardening Tips to Secure your Linux Server
      Linux System Hardening: Top 10 Security Tips
      These resources provide comprehensive guidelines to enhance the security of your Linux systems.
      - Sources
      - You said:
      And if I'm not root, I cannot likely attach a debugger to the process to watch its memory. correct?
      ChatGPT said:
      - ChatGPT
      You're correct; without root privileges, attaching a debugger like GDB to a running process is generally restricted due to security measures in Linux systems. By default, the Linux kernel's Yama security module enforces a policy that prevents non-root users from attaching debuggers to processes they do not own. This is controlled by the /proc/sys/kernel/yama/ptrace_scope setting.
      Understanding ptrace_scope Levels:
      The ptrace_scope setting determines the level of restriction for the ptrace system call, which debuggers use to attach to processes:
      0: No restrictions; any process can be traced by any user.
      1: (Default) A process can only be traced by its parent or by processes with the appropriate permissions.
      2: Only processes with the CAP_SYS_PTRACE capability can use ptrace to trace other processes.
      3: No processes may be traced; ptrace is effectively disabled.
      Modifying ptrace_scope Temporarily:
      To allow a non-root user to attach a debugger to a process they own, you can temporarily modify the ptrace_scope setting:
      echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope
      This command sets ptrace_scope to 0, reducing restrictions. However, this change is temporary and will revert upon reboot.
      GITHUB
      Permanently Modifying ptrace_scope:
      To make this change persistent across reboots, add the following line to a sysctl configuration file, such as /etc/sysctl.d/10-ptrace.conf:
      kernel.yama.ptrace_scope = 0
      Then, apply the changes with:
      sudo sysctl --system
      Granting CAP_SYS_PTRACE Capability to GDB:
      Alternatively, you can grant the CAP_SYS_PTRACE capability to the GDB executable, allowing it to attach to processes without requiring root privileges:
      sudo setcap cap_sys_ptrace=eip /usr/bin/gdb
      This command assigns the necessary capability to GDB. Note that modifying capabilities can have security implications, so proceed with caution.
      GITHUB
      Security Considerations:
      Altering ptrace_scope or granting capabilities can expose the system to potential security risks. It's essential to understand the implications and ensure that only trusted users have access to debugging tools.
      Conclusion:
      While non-root users are generally restricted from attaching debuggers to running processes due to security policies, these restrictions can be adjusted by modifying system settings or granting specific capabilities. However, such changes should be made with caution, considering the security implications for your system.
      Key Takeaways:
      By default, Linux restricts non-root users from attaching debuggers to processes they do not own.
      Modifying the ptrace_scope setting can adjust these restrictions, either temporarily or permanently.
      Granting the CAP_SYS_PTRACE capability to GDB allows non-root users to attach it to processes.
      Always consider the security implications before making such changes.
      For more detailed information, refer to the Yama Linux Security Module documentation.
      - Sources
      - You said:
      then; if keeper runs as a non-root user; then an attacker cannot see the root key; right?
      ChatGPT said:
      - ChatGPT
      Running the keeper process as a non-root user enhances security by limiting the potential impact of a compromised process. In Linux, processes are generally restricted from accessing the memory of other processes unless they have the necessary permissions. This isolation is enforced by the operating system to prevent unauthorized access to sensitive information.
      Key Points:
      Process Isolation: Linux enforces process isolation, preventing non-privileged users from accessing the memory of processes they do not own. This means that, under normal circumstances, an attacker without root privileges cannot directly access the memory of the keeper process to extract sensitive information like the root key.
      ptrace Restrictions: The ptrace system call, commonly used by debuggers to inspect process memory, is restricted by default. Non-root users can typically only attach to processes they own, and even then, certain security modules like Yama can impose further restrictions. This adds an additional layer of protection against unauthorized memory access.
      Considerations:
      Secure Memory Management: While running keeper as a non-root user provides a level of security, it's crucial to implement secure memory management practices. This includes ensuring that sensitive data, such as the root key, is securely erased from memory immediately after use to prevent any potential leakage.
      System Hardening: Beyond user privileges, consider additional system hardening measures, such as enabling security modules like AppArmor or SELinux, to enforce mandatory access controls and further isolate processes.
      Regular Security Audits: Conduct regular security audits and monitoring to detect any unauthorized attempts to access process memory or escalate privileges. This proactive approach helps in maintaining the overall security posture of the system.
      Conclusion:
      Running the keeper process as a non-root user significantly reduces the risk of unauthorized access to its memory space, thereby protecting sensitive information like the root key. However, it's essential to complement this practice with secure memory management, system hardening, and regular security audits to ensure comprehensive protection.
      Key Takeaways:
      Non-root processes are generally isolated from each other's memory spaces, enhancing security.
      Implementing secure memory management and system hardening measures further protects sensitive data.
      Regular security audits are essential to detect and prevent unauthorized access attempts.
      For more detailed information on Linux process isolation and security practices, consider consulting the following resources:
      Introduction to Linux capabilities
      Sandbox (computer security)
      Ptrace
      These resources provide comprehensive guidelines and best practices for enhancing process isolation and security in Linux systems.

    </issue>
    <issue>
      <task>
        Outline a disaster recovery scenario when both nexus and keepers are all down.
      </task>
      <task>The system should work with three keepers by default.</task>
      <task>
        NEW INIT WORKFLOW

        Each keeper starts in NOT_READY state and:
        - Has a unique ID
        - Knows about all other keepers (for mTLS)
        - Has configuration for total keepers and threshold

        Key generation protocol:
        - Each keeper generates its own random contribution
        - They share their contribution with all other keepers
        - Each keeper collects all contributions

        Once all contributions are received, each keeper:
        - XORs all contributions to create the final root key
        - Uses Shamir's Secret Sharing to split it
        - Keeps its own shard
        - Transitions to READY state
        - Securely erases all contributions and the computed root key

        KEEPER CRASH RECOVERY
        - Keeper requests root key from Nexus via mTLS
        - Recomputes all shards using same configuration
        - Keeps its shard, wipes everything else
        - Returns to READY state

        The state transitions are:
        NOT_READY -> CONTRIBUTING -> READY

        Key security properties:

        No single keeper ever has the complete root key
        The final key is as random as any individual keeper's contribution
        Each keeper independently computes the same shards
        After initialization, only the shards remain in memory

        FOR SIMPLICITY WE'll HARD CODE 3 keepers and 2 min shards;
        we will make it configurable later.

        --

        The above flow means we won't need `spike init` command anymore.

        Nexus' states will automatically transition:

        Initial State: PENDING
        - Nexus starts up
        - Begins collecting shards from keepers
        - Cannot serve requests yet

        Shard Collection:
        - Keepers provide their shards via mTLS
        - Nexus stores shards temporarily
        - After each shard addition, checks if threshold is met
        - If keeper is unreachable, we'll try the next one.
        - If after one loop still not enough shards, we backoff and retry.
        - If reconstruction from the shards fail; we backoff and retry.

        Transition to READY:
        Once threshold shards are collected
        - Reconstructs root key using Shamir's combine
        - Clears individual shards from memory
        - Can now serve API requests

        SUPERADMIN BREAK-THE-GLASS EMERGENCY BACKUP
        Security features:
        - Requires super admin SVID for authorization
        - Encrypts backup with AES-GCM using the provided passphrase
        - Creates an audit log entry of backup creation
        - Generates unique backup IDs for tracking
        Emergency backup flow:
        - Creates two shards (requires both for reconstruction)
        - Encrypts them in a JSON structure with metadata
        - Super admin gets an encrypted blob

        SUPERADMIN EMERGENCY RECOVERY
        - In Pilot CLI
        - Super admin provides the passphrase
        - Super admin provides encrypted shard #1
        - Super admin provided encrypted shard #1
      </task>
      <task>Nexus will be NOT_READY until it collects enough shards to create the root key</task>
      <task>
        low prio:
        enable double-encryption when transferring shards between keepers and
        also between nexus and the keeper.
        this will be an additional security measure if the outer mTLS is
        compromised; but if mTLS is compromised, then SPIFFE is compromised
        then the entire identity control plane is compromised, so we'll have
        bigger problems.

      </task>
      <task>
        A secure eras function for key operations

        func secureErase(data []byte) {
        for i := range data {
        data[i] = 0
        }
        }
      </task>
      <task>
        SPIKE KEEPER DR NOTES
        Scenario: Both keepers and Nexus crashed
        Action: best way to recover

        1. If nexus crashes; it can fetch its shards during bootstrap.
        2. If &lt; 2 keepers crash, the keeper can query its peers to reconstruct
        its shard.
        3. Doomsday: Everything crashed all at once--see below:

        This needs to be done before doomsday happens:
        Build Two Distinct Recovery Binaries
        recover1 and recover2, each compiled separately.
        Each binary is associated with a unique SPIFFE ID (e.g., spiffe://mydomain/admin1 and spiffe://mydomain/admin2).
        Enforce SHA-Based Attestation
        Use SPIRE (or a similar SPIFFE implementation) with a “workload attestor” that:
        Verifies the file path and/or SHA-256 hash of each binary.
        Issues an SVID only if the binary matches the known-trusted hash.
        This ensures any tampered binary cannot receive a valid SVID and thus cannot authenticate to Keepers or Nexus.
        Configure Two Non-Root OS Accounts
        On the host machine(s) where these binaries will run:
        admin1 user account is authorized to execute recover1
        admin2 user account is authorized to execute recover2
        Deny both accounts any privileged or sudo access to ensure they can’t attach debuggers or read each other’s processes.
        Threshold Cryptography (Optional, but Recommended)
        Split the root key into (at least) 2-of-2 or 2-of-3 shards stored in separate Keepers.
        Each Keeper requires a valid SVID corresponding to recover1 or recover2 before releasing its shard.
        Superadmin Role for DR Endpoint
        If using the approach where Nexus has a special recovery endpoint (/api/v1/recover):
        Superadmin sets an environment variable like __SPIKE_NEXUS_DANGEROUSLY_ENABLE_RECOVERY_ENDPOINT__="yes" and restarts Nexus.
        The recovery endpoint opens for a short time window (e.g., 15 minutes).
        Nexus automatically seals the endpoint once two shards arrive or time expires.
        Only superadmin can enable or disable this endpoint.
        Recover1 and Recover2 Execution
        Admin1 logs into the host as admin1 and runs ./recover1.
        recover1 attests via SPIRE, proving it’s the correct untampered binary.
        It receives its short-lived cert (SVID).
        It contacts Keeper A (or whichever Keeper enforces “admin1’s shard”) over mTLS, requests the shard.
        It then forwards (proxies) the shard directly to Nexus’s /api/v1/recover endpoint (if that’s your chosen design) without printing to screen.
        Admin2 does the same process with ./recover2 (and possibly Keeper B), sending its shard to Nexus.
        Shard Reconstruction in Nexus
        Once Nexus sees two shards, it rebuilds the root key in memory.
        If the DR scenario is complete, Nexus automatically seals the endpoint or the superadmin restarts Nexus without the environment variable, removing the DR capability until next time.
        Logs and Auditing
        Each Keeper logs which SPIFFE ID requested a shard and when.
        Nexus logs the time-limited endpoint usage.
        OS logs track which user executed recover1 and recover2.
        Threat Analysis

        Below are the most pertinent threats and how your design addresses them.
        Tampering with the Recover Binaries
        Threat: An attacker replaces or modifies recover1 or recover2 to exfiltrate shards.
        Mitigation: SHA-based attestation prevents the tampered binary from receiving a valid SVID, so Keepers won’t release shards.
        Residual Risk: If an attacker can also compromise your SPIRE agent or attestation pipeline, they might forge attestation. This requires deeper OS or supply chain compromise.
        Single Admin or Rogue Insider
        Threat: One admin obtains all shards.
        Mitigation: The threshold approach requires two binaries run by two different OS users. A single admin cannot run both binaries unless they break into the other’s account.
        Residual Risk: Collusion between Admin1 and Admin2, or compromised credentials for both accounts.
        Memory Snooping or Debugging
        Threat: An admin tries to attach a debugger or scan memory to extract a shard in plaintext.
        Mitigation: Non-root accounts typically cannot debug processes owned by another user. Also, the ephemeral shard is never printed or written to disk.
        Residual Risk: If an attacker escalates to root or uses a kernel exploit, they could read process memory. This is a broader OS-level compromise scenario.
        Network Eavesdropping
        Threat: An attacker captures traffic between the Keeper and Nexus to steal shards.
        Mitigation: mTLS ensures all traffic is encrypted, with certificate pinning (SPIFFE). An attacker cannot decrypt the channel without compromising an endpoint.
        Residual Risk: Endpoint compromise can still reveal plaintext (in memory), but purely passive network sniffing is defeated.
        OS or Kernel-Level Exploits
        Threat: A sophisticated attacker escalates privileges or patches the kernel to intercept memory buffers.
        Mitigation: Regular OS patching, secure boot, minimal privileges, and auditing. In high-security environments, hardware enclaves (SGX/SEV/TPM) could further reduce the risk.
        Residual Risk: A zero-day or advanced persistent threat can bypass typical OS protections.
        DR Endpoint Abuse
        Threat: Malicious user repeatedly enables the /api/v1/recover to gain key shards.
        Mitigation: Only superadmin can enable it, and it’s time-limited. All usage is audited.
        Residual Risk: A compromised superadmin or an attacker who obtains superadmin privileges can enable it at will.
        Collusion and Insider Risk
        Threat: Admin1, Admin2, and/or superadmin collude to steal the root key for malicious purposes.
        Mitigation: Multi-man or threshold cryptography is designed to reduce single-person risk. However, it does not solve a scenario where all relevant parties collude. Additional external controls or background checks might be needed in extremely sensitive contexts.
        Conclusion of the Threat Analysis
        Overall Security Posture: Very strong for standard enterprise or production-level needs, as you’ve implemented:
        Separate non-root accounts
        SHA-based binary attestation
        mTLS with SPIFFE IDs
        Threshold cryptography requiring two distinct shards
        Time-limited DR endpoint behind a superadmin role
        Remaining Weak Points:
        Full OS-level compromise can still subvert in-memory secrets.
        Collusion or social engineering might bypass the two-admin requirement.
        Advanced nation-state or zero-day kernel attacks are always a possibility in extreme threat models.
        In typical practice, these protections raise the bar so high that an attacker would likely look for easier vectors (e.g., phishing credentials for multiple admins, or attacking other infrastructure) rather than trying to break your robust DR design.

        ----

        Keeper Distribution
        Suppose you have 100 Keepers: K1, K2, K3, …, K100.
        “recover1” is coded to fetch shards from the odd-numbered Keepers (K1, K3, K5, …, K99).
        “recover2” is coded to fetch shards from the even-numbered Keepers (K2, K4, K6, …, K100).
        Threshold Example
        Let’s you need
        t=10 total shards out of the 100 to reconstruct the key.
        “recover1” can fetch 5 shards from any 5 of the odd-numbered Keepers.
        “recover2” can fetch the remaining 5 shards from 5 even-numbered Keepers.
        Each binary alone only gathers half the threshold. Neither can unilaterally reconstruct the key.
        Non-Root + SHA-Based Attestation
        The same security controls apply: each binary has its own SPIFFE ID, code hash attestation, and runs under a different non-root user.
        Even though each binary interacts with multiple Keepers, the principle remains that a single compromised user or binary cannot gather enough shards.
        3. Ensuring You Don’t Overfetch Shards

        Fetch Exactly the Required Number
        If your threshold is 10, “recover1” only fetches 5 shards, “recover2” only fetches 5 shards.
        You don’t want “recover1” to be able to request 10 shards from all 50 odd Keepers. That would defeat the threshold separation.
        How to Enforce
        Each Keeper can have a policy: “This Keeper will only release a shard to recover1 if it sees that recover1 has not yet fetched more than 5 shards total.” Or more simply, you trust that “recover1” stops after 5.
        Alternatively, you can design a server-side policy or some orchestration logic that only issues 5 valid partial shares to “recover1,” even if it tries to talk to more Keepers.


      </task>
      <task>
        TODO:
        Add more robust error handling specific to CIRCL's error types?
        Add validation for the key size that aligns with CIRCL's requirements?
        Update the emergency backup code to use CIRCL as well?

        TODO: talk
        talk about managing state (transitiong keer to other states)
        disaster recovery: keeper crashes
        disaster recovery: nexus crashes
        VO

      </task>
      <task>
        Nexus will have a background process to reconstruct the root key in memory.

        Until the root key is constructed; any call other than `spike init` will
        result in "please initialize spike first" warning.


        After init workflow is done Nexus, will set a special
        tombstone secret to indicate that it is initialized.
        The former initialization logic will be discarded.
          Nexus Status:
            pending: If it cannot connect to keepers.
            message: "make sure keepers are up and nexus it configured to talk to them"

            uninitialized: all keepers connected so far returns an empty shard
            message: "please `spike init` to initialize spike"

            initializing: at least one keeper returns a shard, but not enough to create root key
            message: "system initializing, please wait"

            initialized: root key fully constructed; no need to use keepers for now
            message: "OK"

            ---
        Case: Nexus crashes


        uninitialized/null | initializing | ready |

      </task>
    </issue>
  </next>
  <low-hanging-fruits>
    <issue>
      enable SQLlite by default
      and test it (ie. crash nexus and ensure both secrets and policies can be recovered)
    </issue>
    <issue>
      Multiple Keeper instances will be required for fan-in fan-out of the
      shards.

      configure the current system to work with multiple keepers.
      the demo setup should initialize 3 keepers by default.
      the demo setup should use sqlite as the backing store by default.
    </issue>
    <issue>
      Install Keycloak locally and experiment with it.
      This is required for "named admin" feature.
    </issue>
    <issue>
      One way token flow;
      keeper provides the rootkey to nexus;
      nexus init pushes root key to keeper.
      that's it.
    </issue>
    <issue>
      If SPIKE is not initialized; `spike` or `spike --help` should display
      a reminder to initialize SPIKE first and exit.
    </issue>
    <issue>
      The paths that we set in get put ...etc should look like a unix path.
      it will require sanitization!
      Check how other secrets stores manage those paths.
    </issue>
    <issue>
      read policies from a yaml or a json file and create them.
    </issue>
    <issue>
      have sqlite as the default backing store.
      (until we implement the S3 backing store)
    </issue>
  </low-hanging-fruits>
  <later>
    <issue>
      set sqlilite on by default and make sure everything works.
    </issue>
    <issue>
      volkan@spike:~/Desktop/WORKSPACE/spike$ spike secret get /db
      Error reading secret: post: Problem connecting to peer

      ^ I get an error instead of a "secret not found" message.
    </issue>
    <issue>
      this is from SecretReadResponse, so maybe its entity should be somewhere
      common too.
      return &amp;data.Secret{Data: res.Data}, nil
    </issue>
    <issue>
      these may come from the environment:

      DataDir: ".data",
      DatabaseFile: "spike.db",
      JournalMode: "WAL",
      BusyTimeoutMs: 5000,
      MaxOpenConns: 10,
      MaxIdleConns: 5,
      ConnMaxLifetime: time.Hour,
    </issue>
  </later>
  <reserved>
    <issue waitingFor="shamir-to-be-implemented-first">
      use case: shamir
      1. `spike init` verifies that there are 3 healthy keeper instances.
      it creates a shard of 3 shamir secrets (2 of which will be enough to
      reassemble the root key) send each share to each keeper.
      2. SPIKE nexus regularly polls all keepers and if it can assemble a secret
      all good.
      3. `spike init` will also save the 2 shards (out of 3) in
      `~/.spike/recovery/*`
      The admin will be "highly encouraged" do delete those from the machine and
      securely back up the keys and distribute them to separate people etc.
      [2 and 3 are configurable]
    </issue>
    <issue waitingFor="shamir-to-be-implemented">
      <workflow>
        1. `spike init` initializes keeper(s). From that point on, SPIKE Nexus
        pulls the root key whenever it needs it.
        2. nexus and keeper can use e2e encryption with one time key pairs
        to have forward secrecy and defend the transport in the VERY unlikely
        case of a SPIFFE mTLS breach.
        3. ability for nexus to talk to multiple keepers
        4. ability for a keeper to talk to nexus to recover its root key if it
        loses it.
        5. abiliy for nexus to talk to and initialize multiple keepers.
        (phase 1: all keepers share the same key)
        6. `spike init` saves its shards (2 out of 3 or smilar) to
        `~/.spike/recovery/*`
        The admin will be "highly encouraged" to delete those from the machine
        and
        securely back up the keys and distribute them to separate people etc
        `spike init` will also save the primary key used in the shamir's secret
        sharing
        to `~/.spike/recovery/*` (this is not as sensitive as the root key, but
        still
        should be kept safe)
        - it is important to note that, without the recovery material, your only
        opiton
        to restore the root key relies on the possibility that more than N
        keepers remain
        operational at all times. -- that's a good enough possibility anyway
        (say 5 keepers in 3 AZs, and you need only 2 to recover the root key;
        then it will
        be extremely unlikely for all of them to go down at the same time)
        so in an ideal scenario you save your recovery material in a secure
        encrypted enclave
        and never ever use it.
        7. `spike recover` will reset a keeper cluster by using the recovery
        material.
        `spike recover` will also recover the root key.
        to use `spike recover` you will need a special SVID (even a super admin
        could not use it
        without prior authorization)
        the SVID who can execute `spike recover` will not be able to execute
        anything else.
        8. At phase zero, `spike recover` will just save the root key to disk,
        also mentioning that it's not secure and the key will be stored safely
        and wiped from the disk.
        9. maybe double encrypt keeper-nexus communication with one-time key
        pairs because
        the root key is very sensitive and we would want to make sure it's
        secure even
        if the SPIFFE mTLS is compromised.
      </workflow>
      <details>
        say user sets up 5 keeper instances.
        in nexus, we have a config
        keepers:
        - nodes: [n1, n2, n3, n4, n5]
        nexus can reach out with its own spiffe id to each node in the list. it
        can
        call the assembly lib with whatever secrets it gets back, as it gets
        them back,
        and so long as it gets enough, "it just works"

        recovery could even be, users have a copy of some of the keeper's
        secrets.
        they rebuild a secret server and load that piece back in. nexus then can
        recover.
        that api could also allow for backup configurations
      </details>
      <docs>
        WAITINGFOR: shamir to be implemented

        To documentation (Disaster Recovery)

        Is it like
        Keepers have 3 shares.
        I get one share
        you get one share.
        We keep our shares secure.
        none of us alone can assemble a keeper cluster.
        But we two can join our forces and do an awesome DR at 3am in the
        morning if needed?

        or if your not that paranoid, you can keep both shares on one
        thumbdrive, or 2
        shares on two different thumbdrives in two different safes, and rebuild.

        it gives a lot of options on just how secure you want to try and make
        things vs
        how painful it is to recover
      </docs>
    </issue>
    <issue waitingFor="shamir-to-be-implemented">
      func RouteInit(
      w http.ResponseWriter, r *http.Request, audit *log.AuditEntry,
      ) error {
      // This flow will change after implementing Shamir Secrets Sharing
      // `init` will ensure there are enough keepers connected, and then
      // initialize the keeper instances.
      //
      // We will NOT need the encrypted root key; instead, an admin user will
      // fetch enough shards to back up. Admin will need to provide some sort
      // of key or password to get the data in encrypted form.
    </issue>
  </reserved>
  <immediate-backlog>
  </immediate-backlog>
  <runner-up>
    <issue>
      double-encryption of nexus-keeper comms (in case mTLS gets compromised, or
      SPIRE is configured to use an upstream authority that is compromised, this
      will provide end-to-end encryption and an additional layer of security
      over
      the existing PKI)
    </issue>
    <issue>
      Minimally Delightful Product Requirements:
      - A containerized SPIKE deployment
      - A Kubernetes SPIKE deployment
      - Minimal policy enforcement
      - Minimal integration tests
      - A demo workload that uses SPIKE to test things out as a consumer.
      - A golang SDK (we can start at github/zerotohero-dev/spike-sdk-go
      and them move it under spiffe once it matures)
    </issue>
    <issue>
      Kubernetification
    </issue>
    <issue>
      v.1.0.0 Requirements:
      - Having S3 as a backing store
    </issue>
    <issue>
      Consider a health check / heartbeat between Nexus and Keeper.
      This can be more frequent than the root key sync interval.
    </issue>
    <issue>
      Unit tests and coverage reports.
      Create a solid integration test before.
    </issue>
    <issue>
      Test automation.
    </issue>
    <issue>
      Assigning secrets to SPIFFE IDs or SPIFFE ID prefixes.
    </issue>
  </runner-up>
  <backlog>
    <issue kind="v1.0-requirement">
      - Run SPIKE in Kubernetes too.
    </issue>

    <issue kind="v1.0-requirement">
      - Postgres support as a backing store.
    </issue>

    <issue kind="v1.0-requirement">
      - Ability to channel audit logs to a log aggregator.
    </issue>

    <issue kind="v1.0-requirement">
      - OIDC integration: Ability to connect to an identity provider.
    </issue>

    <issue kind="v1.0-requirement">
      - ESO (External Secrets Operator) integration
    </issue>

    <issue kind="v1.0-requirement">
      - An ADMIN UI (linked to OIDC probably)
    </issue>

    <issue kind="v1.0-requirement">
      - Ability to use the RESTful API without needing an SDK.
      That could be hard though since we rely on SPIFFE authentication and
      SPIFFE workload API to gather certs: We can use a tool to automate that
      part. But it's not that hard either if I know where my certs are:
      `curl --cert /path/to/svid_cert.pem --key /path/to/svid_key.pem
      https://mtls.example.com/resource`
    </issue>

    <issue kind="v1.0-requirement">
      > 80% unit test coverage
    </issue>

    <issue kind="v1.0-requirement">
      Fuzzing for the user-facing API
    </issue>

    <isssue kind="v1.0-requirement">
      100% Integration test (all features will have automated integration tests
      in all possible environments)
    </isssue>
    <issue>
      By design, we regard memory as the source of truth.
      This means that backing store might miss some secrets.
      Find ways to reduce the likelihood of this happening.
      1. Implement exponential retries.
      2. Implement a health check to ensure backing store is up.
      3. Create background jobs to sync the backing store.
    </issue>
    <issue>
      Test the db backing store.
    </issue>
    <issue>
      Ability to add custom metadata to secrets.
    </issue>
    <issue>
      We need use cases in the website
      - Policy-based access control for workloads
      - Secret CRUD operations
      - etc
    </issue>
    <issue>
      Fleet management:
      - There is a management plane cluster
      - There is a control plane cluster
      - There are workload clusters connected to the control plane
      - All of those are their own trust domains.
      - There is MP-CP connectivity
      - There is CP-WL connectivity
      - MP has a central secrets store
      - WL and CP need secrets
      - Securely dispatch them without "ever" using Kubernetes secrets.
      - Have an alternative that uses ESO and a restricted secrets namespace
      that no one other than SPIKE components can see into.
    </issue>
    <issue>
      To docs:
      how do we manage the root key.
      i.e., it never leaves the memory and we keep it alive via replication.
    </issue>
    <issue>
      API for SPIKE nexus to save its shard encrypted with a passphrase for
      emergency backup
      This will be optional; and admin will be advised to save it securely
      outside the machine.
      (requires the shamir secret sharing to be implemented)
    </issue>
    <issue>
      Postgresql support for backing store.
    </issue>
    <issue>
      maybe a default auditor SPIFFEID that can only read stuff (for Pilot;
      not for named admins; named admins will use the policy system instead)
    </issue>
    <issue>
      Optionally not create tables and other ddl during backing store creation
    </issue>
    <issue>
      What if a keeper instance crashes and goes back up?
      if there is an "initialized" Nexus; it can hint nexus to send its share
      again.
    </issue>

    <issue>
      Think about DR scenarios.
    </issue>
    <issue>
      SPIKE Pilot to ingest a policy YAML file(s) to create policies.
      (similar to kubectl)
    </issue>
    <issue>
      - SPIKE Keep Sanity Tests
      - Ensure that the root key is stored in SPIKE Keep's memory.
      - Ensure that SPIKE Keep can return the root key back to SPIKE Nexus.
    </issue>
    <issue>
      Demo: root key recovery.
    </issue>

    <issue>
      If there is a backing store, load all secrets from the backing store
      upon crash, which will also populate the key list.
      after recovery, all secrets will be there and the system will be
      operational.
      after recovery admin will lose its session and will need to re-login.
    </issue>
    <issue>
      Test edge cases:
      * call api method w/o token.
      * call api method w/ invalid token.
      * call api method w/o initializing the nexus.
      * call init twice.
      * call login with bad password.
      ^ all these cases should return meaningful errors and
      the user should be informed of what went wrong.
    </issue>
    <issue>
      Try SPIKE on a Mac.
    </issue>
    <issue>
      Try SPIKE on an x-86 Linux.
    </issue>

    <issue>
      based on the following, maybe move SQLite "create table" ddls to a
      separate file.
      Still a "tool" or a "job" can do that out-of-band.

      update: for SQLite it does not matter as SQLite does not have a concept
      of RBAC; creating a db is equivalent to creating a file.
      For other databases, it can be considered, so maybe write an ADR for that.

      ADR:

      It's generally considered better security practice to create the schema
      out-of-band (separate from the application) for several reasons:

      Principle of Least Privilege:

      The application should only have the permissions it needs for runtime
      (INSERT, UPDATE, SELECT, etc.)
      Schema modification rights (CREATE TABLE, ALTER TABLE, etc.) are not
      needed during normal operation
      This limits potential damage if the application is compromised


      Change Management:

      Database schema changes can be managed through proper migration tools
      Changes can be reviewed, versioned, and rolled back if needed
      Prevents accidental schema modifications during application restarts


      Environment Consistency:

      Ensures all environments (dev, staging, prod) have identical schemas
      Reduces risk of schema drift between environments
      Makes it easier to track schema changes in version control
    </issue>


    <qa>
      <issue>
        - SPIKE Nexus Sanity Tests
        - Ensure SPIKE Nexus caches the root key in memory.
        - Ensure SPIKE Nexus reads from SPIKE keep if it does not have the root
        key.
        - Ensure SPIKE Nexus saves the encrypted root key to the database.
        - Ensure SPIKE Nexus caches the user's session key.
        - Ensure SPIKE Nexus removes outdated session keys.
        - Ensure SPIKE Nexus does not re-init (without manual intervention)
        after
        being initialized.
        - Ensure SPIKE Nexus adheres to the bootstrapping sequence diagram.
        - Ensure SPIKE Nexus backs up the admin token by encrypting it with the
        root
        key and storing in the database.
        - Ensure SPIKE Nexus stores the initialization tombstone in the
        database.
      </issue>
      <issue>
        - SPIKE Pilot Sanity Tests
        - Ensure SPIKE Pilot denies any operation if SPIKE Nexus is not
        initialized.
        - Ensure SPIKE Pilot can warn if SPIKE Nexus is unreachable
        - Ensure SPIKE Pilot does not indefinitely hang up if SPIRE is not
        there.
        - Ensure SPIKE Pilot can get and set a secret.
        - Ensure SPIKE Pilot can do a force reset.
        - Ensure SPIKE Pilot can recover the root password.
        - Ensure that after `spike init` you have a password-encrypted root key
        in the db.
        - Ensure that you can recover the password-encrypted root key.
      </issue>
    </qa>



  </backlog>
  <future>
    <issue>
      multiple keeper clusters:

      keepers:
      - nodes: [n1, n2, n3, n4, n5]
      - nodes: [dr1, dr2]

      if it cant assemble back from the first pool, it could try the next
      pool, which could be stood up only during disaster recovery.
    </issue>
    <issue>
      a tool to read from one cluster of keepers to hydrate a different
      cluster of keepers.
    </issue>

    <issue>
      since OPA knows REST, can we expose a policy evaluation endpoint to
      help OPA augment/extend SPIKE policy decisions?
    </issue>
    <issue>
      maybe create an interface for kv, so we can have thread-safe variants too.
    </issue>

    <issue>
      maybe create a password manager tool as an example use case
    </issue>

    <issue>
      A `stats` endpoint to show the overall
      system utilization
      (how many secrets; how much memory, etc)
    </issue>
    <issue>
      maybe inspire admin UI from keybase
      https://keybase.io/v0lk4n/devices
      for that, we need an admin ui first :)
      for that we need keycloak to experiment with first.
    </issue>

    <issue>
      the current docs are good and all but they are not good for seo; we might
      want to convert to something like zola later down the line
    </issue>

    <issues>
      wrt ADR-0014:
      Maybe we should use something S3-compatible as primary storage
      instead of sqlite.
      But that can wait until we implement other features.

      Besides, Postgres support will be something that some of the stakeholders
      want to see too.
    </issues>


    <issue>
      SPIKE Dev Mode:

      * Single binary
      * `keeper` functionality runs in memory
      * `nexus` uses an in-memory store, and its functionality is in the single
      binary too.
      * only networking is between the binary and SPIRE Agent.
      * For development only.

      The design should be maintainable with code reuse and should not turn into
      maintaining two separate projects.
    </issue>
    <issue>
      rate limiting to api endpoints.
    </issue>
    <issue>
      * super admin can create regular admins and other super admins.
      * super admin can assign backup admins.
      (see drafts.txt for more details)
    </issue>
    <issue>
      Each keeper is backed by a TPM.
    </issue>
    <issue>
      Do some static analysis.
    </issue>
    <to-plan>
      <issue>
        S3 (or compatible) backing store
      </issue>
      <issue>
        File-based backing store
      </issue>
      <issue>
        In memory backing store
      </issue>
      <issue>
        Kubernetes Deployment
      </issue>
    </to-plan>
    <issue>
      Initial super admin can create other admins.
      So that, if an admin leaves, the super admin can delete them.
      or if the password of an admin is compromised, the super admin can
      reset it.
    </issue>
    <issue>
      - Security Measures (SPIKE Nexus)
      - Encrypting the root key with admin password is good
      Consider adding salt to the password encryption
      - Maybe add a key rotation mechanism for the future
    </issue>
    <issue>
      - Error Handling
      - Good use of exponential retries
      - Consider adding specific error types/codes for different failure
      scenarios
      - Might want to add cleanup steps for partial initialization failures
    </issue>
    <issue>
      Ability to stream logs and audit trails outside of std out.
    </issue>
    <issue>
      Audit logs should write to a separate location.
    </issue>
    <issue>
      Create a dedicated OIDC resource server (that acts like Pilot but exposes
      a
      restful API for things like CI/CD integration.
    </issue>
    <issue>
      HSM integration (i.e. root key is managed/provided by an HSM, and the key
      ever leaves the trust boundary of the HSM.
    </issue>
    <issue>
      Ability to rotate the root key (automatic via Nexus).
    </issue>
    <issue>
      Ability to rotate the admin token (manual).
    </issue>
    <issue>
      Encourage to create users instead of relying on the system user.
    </issue>
  </future>
</stuff>