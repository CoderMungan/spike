1. `spike init` verifies that there are 3 healthy keeper instances.
   it creates a shard of 3 shamir secrets (2 of which will be enough to reassemble the root key)
   send each share to each keeper.
2. SPIKE nexus regularly polls all keepers and if it can assemble a secret all good.
3. `spike init` will also save the 2 shards (out of 3) in `~/.spike/recovery/*`
   The admin will be "highly encouraged" do delete those from the machine and
   securely back up the keys and distribute them to separate people etc.
[2 and 3 are configurable]

What if a keeper instance crashes and goes back up?
if there is an "initialized" Nexus; it can hint nexus to send its share again.

---
though, if you just gave it the list of keepers, it would know how many to support then, and maybe just one flag for how many need to be online before recovery, and default to a reasonable subset?
default= (n/2+1)?
that would even let someone for dev purposes just give it 1
not very secure, but easy to stand up

---


-----
* Split the root key between 5 keeper instances.
* Upon crash, nexus can talk to 3 of the keepers to recover the root key.
* 5 and 3 are configurable.
* If a keeper crashes, it will hint nexus to get it share asap (keeper sends
  an /v1/hydrate request to nexus; and nexus will send the partial key that
  keeper needs.
 * If > 2 keepers crash AND nexus crashes too, then "womp womp", too bad.
   we'll need admin's recovery token as a "break the glass" approach to manually
   recover the system.
 * For security, admin's recovery token will be split into 5 shards too.
 * Only 3 of those shards will be sufficient to recover the root key.
 * Root key ALWAYS stored in memory, we don't give the root key to the admin;
   the shamir-sharded tokens that admin has are used to decrypt and recover the
   root key. -- this means even an admin cannot decrypt secrets without SPIKE.


Either increase the number of keepers (i.e. 15 keepers, and 3 of those keepers can assemble a key, so its’ very unlikely for all of them to crash at the same time — I still feel 5 (or 7 for extra paranoid) is a good enough ballpark number)
Ooor use a TPM and use its key derivation function to secure the share that the keepers keep.

chat with Kevin
@Kevin Fox
 here’s my idea (jotted between meetings) about splitting root key and recovering it.
At a high level, there are two modes of operation
Keeper instances form a quorum and reconfigure the system when shit happens.
When the quorum cannot be formed (i.e. >N keepers are down and nexus has crashed too) then we have a “break-the-glass” admin recovery token that can re-initialize the system.
The details are roughly like this, but I guess I’ll have more clarity and see the gaps better when I actually start coding things:
* Split the root key between 5 keeper instances.
* Upon crash, nexus can talk to 3 of the keepers to recover the root key.
* 5 and 3 are configurable.
* If a keeper crashes, it will hint nexus to get it share asap (keeper sends
 an /v1/hydrate request to nexus; and nexus will send the partial key that
 keeper needs.
 * If > 2 keepers crash AND nexus crashes too, then “womp womp”, too bad.
  we’ll need admin’s recovery token as a “break the glass” approach to manually
  recover the system.
 * For security, admin’s recovery token will be split into 5 shards too.
 * Only 3 of those shards will be sufficient to recover the root key.
 * Root key ALWAYS stored in memory, we don’t give the root key to the admin;
  the shamir-sharded tokens that admin has are used to decrypt and recover the
  root key. -- this means even an admin cannot decrypt secrets without SPIKE.
^
How does it look?







Kevin Fox
  12:13 PM
agree to the general idea. but maybe through basic config, end up being more flexable....





12:13
say user sets up 5 keeper instances.
12:14
in nexus, we have a config
keepers:
  - nodes: [n1, n2, n3, n4, n5]


Volkan Ozcelik
  12:14 PM
(ah I see, something terraform…ey)


Kevin Fox
  12:15 PM
nexus can reach out with its own spiffe id to each node in the list. it can call the assembly lib with whatever secrets it gets back, as it gets them back, and so long as it gets enough, "it just works"
:+1:
1

12:15
recovery could even be, users have a copy of some of the keeper's secrets. they rebuild a secret server and load that piece back in. nexus then can recover.
12:15
that api could also allow for backup configurations
12:16
keepers:
  - nodes: [n1, n2, n3, n4, n5]
  - nodes: [dr1, dr2]
12:16
if it cant assemble back from the first pool, it could try the next pool, which could be stood up only during disaster recovery.
12:17
(thinking out loud. not sure if its a good idea or not. but its rather simple in design, which is nice)
12:17
could even:
keepers:
  - nodes: [n1, n2, n3, n4, n5]
  - nodes: [dr]
If you want just one person to have the entire recovery secret


Volkan Ozcelik
  12:18 PM
Need to think… but sounds good on paper.


Kevin Fox
  12:19 PM
could even use that mechanism to go from a less secure, 3 node setup, migrate to a 5 node setup, and then remove the 3 node setup.


Volkan Ozcelik
  12:19 PM
In your model, do keepers also create the secret, or is it another admin that initializes the keeper quorum and nexus just blindly fetches from keepers?
New


Kevin Fox
  12:19 PM
thinking maybe an admin runs a tool, that in memory generates the secret, takes in the same keeper list, and pushes just the bits to each keeper using the admins spiffe id. only the admin could write to the keepers.
12:20
nexus could read the bits and reassemble to memory and get that secret back. (edited)
12:21
same tool could pull the secret maybe, reassemble, spit in different shares, and push to a second pool of keepers, for migrating to more secure setups
12:22
or, maybe keepers can have multiple secrets with different pools of shares. may be easier, but more complicated. haven't though much about that.


Volkan Ozcelik
  12:22 PM
That also means, as an admin, I can push the same sharded secret to a 5 node production keeper(set) and a 3 node dr keeper(set) that I only bring up if things go bad.
ooor. I can have… I dunno 3 separate DR clusters that are totally disconnected from the network; all of them in separate avaiability zones; all of them reporting their health etc. so I am confident that there is at least one cluster that I can recover the secret from if I want to.
:+1:
1



Kevin Fox
  12:22 PM
and can even shutdown the dr cluster once initialized and keep it safe until ready
12:23
that also provides a nice place for even more share security.
12:23
say you had tpm's in the keeper nodes.
12:23
the share it gets can be encrypted with the tpm, and then put on disk encrypted. keeper can unencrypt it back with the tpm on start.
12:23
share doenst hit disk unencrypted then. (edited)
12:24
all sorts of stuff can be done at the keeper level then I think.
12:24
could be a place for some fancy/interesting plugins. but, can be kept simple for now


Volkan Ozcelik
  12:25 PM
yup. that would mean even if the keeper crashes, it can recover its state from TPM.
And since we assume that stealing TPM requires physical access and lots of jail time we would be secure enough :slightly_smiling_face:

---

Normal Operation:
1. Keeper cluster has 5 keepers, needs 3 for threshold
2. Each keeper holds its shard in memory only

Backup Setup:
1. Person1 ---(public key1)---> Orchestrator
2. Person2 ---(public key2)---> Orchestrator
3. Person3 ---(public key3)---> Orchestrator
4. Person4 ---(public key4)---> Orchestrator
5. Person5 ---(public key5)---> Orchestrator

Orchestrator:
- Creates new 3-of-5 sharing of the keeper shards
- Encrypts share1 with public key1
- Encrypts share2 with public key2
- ...and so on

Recovery:
1. At least 3 people decrypt their shares with private keys
2. They use these to bootstrap a new keeper cluster


=====

Is it like
Keepers have 3 shares.
I get one share
you get one share.
We keep our shares secure.
none of us alone can assemble a keeper cluster.
But we two can join our forces and do an awesome DR at 3am in the morning if needed?


----
or if your not that paranoid, you can keep both shares on one thumbdrive, or 2 shares on two different thumbdrives in two different safes, and rebuild.
----
it gives a lot of options on just how secure you want to try and make things vs how painful it is to recover

/ Create a new group (using ed25519 as an example)
	g := ed25519.NewGroup()

	// Create a secret value (for example purposes, we'll use a random scalar)
	secret := g.RandomScalar(rand.Reader)

	// Parameters for the secret sharing scheme:
	// t = 2: need 3 shares to reconstruct (threshold + 1)
	// n = 5: total number of shares to create
	threshold := uint(2)
	numShares := uint(5)

	// Create a new secret sharing instance
	ss := New(rand.Reader, threshold, secret)

	// Generate n shares
	shares := ss.Share(numShares)

	// Optional: Create a commitment to verify shares later
	commitment := ss.CommitSecret()

	fmt.Printf("Created %d shares with threshold %d\n", numShares, threshold)
	for i, share := range shares {
		// Verify each share against the commitment
		isValid := Verify(threshold, share, commitment)
		fmt.Printf("Share %d: Valid=%v\n", i+1, isValid)
	}

	// Simulate recovering the secret with different subsets of shares

	// Try with insufficient shares (should fail)
	insufficientShares := shares[:2]
	recoveredSecret, err := Recover(threshold, insufficientShares)
	if err != nil {
		fmt.Printf("Expected error with insufficient shares: %v\n", err)
	}

	// Try with sufficient shares (should succeed)
	sufficientShares := shares[:3]
	recoveredSecret, err = Recover(threshold, sufficientShares)
	if err != nil {
		log.Fatalf("Unexpected error: %v", err)
	}

	// Verify the recovered secret matches the original
	if recoveredSecret.IsEqual(secret) {
		fmt.Println("Successfully recovered the secret!")
	} else {
		fmt.Println("Error: Recovered secret does not match original!")
	}

	// You can also create shares with specific IDs
	customID := g.NewScalar().SetUint64(42)
	customShare := ss.ShareWithID(customID)
	isValid := Verify(threshold, customShare, commitment)
	fmt.Printf("Custom share with ID 42: Valid=%v\n", isValid)



------




package auth

import (
	"crypto/hmac"
	"crypto/rand"
	"encoding/hex"
	"errors"
	"net/http"

	"github.com/spiffe/spike/internal/entity/v1/reqres"
	"github.com/spiffe/spike/internal/log"
	"github.com/spiffe/spike/internal/net"
)

// checkHmac verifies if the provided password hash matches the expected hash
// using HMAC comparison. It handles unauthorized access attempts by responding
// with appropriate HTTP status codes and error messages.
//
// Parameters:
//   - ph: The provided password hash as a byte slice
//   - b: The expected password hash to compare against
//   - w: HTTP ResponseWriter for sending error responses
//
// Returns:
//   - error: nil if the HMAC verification succeeds, or an error describing
//     the failure
//
// The function will return an "invalid password" error and respond with
// HTTP 401 Unauthorized if the HMAC comparison fails.
func checkHmac(ph, b []byte, w http.ResponseWriter) error {
	if hmac.Equal(ph, b) {
		log.Log().Info("checkHmac", "msg", "Valid password")
		return nil
	}

	log.Log().Info("checkHmac", "msg", "Invalid password")

	responseBody := net.MarshalBody(reqres.AdminLoginResponse{
		Err: reqres.ErrUnauthorized}, w)
	if responseBody == nil {
		return errors.New("failed to marshal response body")
	}

	net.Respond(http.StatusUnauthorized, responseBody, w)
	log.Log().Info("routeAdminLogin", "msg", "unauthorized")
	return errors.New("invalid password")
}

// decodePasswordHash decodes a hexadecimal password hash string into bytes.
// It handles decoding errors by responding with appropriate HTTP status
// codes and error messages.
//
// Parameters:
//   - passwordHash: Hexadecimal string representation of the password hash
//   - w: HTTP ResponseWriter for sending error responses
//
// Returns:
//   - []byte: Decoded password hash bytes
//   - error: nil if decoding succeeds, or an error describing the failure
//
// The function will respond with HTTP 500 Internal Server Error and return
// an empty byte slice if the decoding fails or if response marshaling fails.
func decodePasswordHash(
	passwordHash string, w http.ResponseWriter,
) ([]byte, error) {
	b, err := hex.DecodeString(passwordHash)
	if err != nil {
		log.Log().Error("decodePasswordHash",
			"msg", "Problem decoding password hash",
			"err", err.Error())

		responseBody := net.MarshalBody(reqres.AdminLoginResponse{
			Err: reqres.ErrServerFault}, w)
		if responseBody == nil {
			return []byte{}, errors.New("failed to marshal response body")
		}

		net.Respond(http.StatusInternalServerError, responseBody, w)
		log.Log().Info("decodePasswordHash", "msg", "OK")
		return []byte{}, errors.New("failed to decode password hash")
	}
	return b, nil
}

// decodeSalt decodes a hexadecimal salt string into bytes.
// It handles decoding errors by responding with appropriate HTTP status codes
// and error messages.
//
// Parameters:
//   - salt: Hexadecimal string representation of the salt
//   - w: HTTP ResponseWriter for sending error responses
//
// Returns:
//   - []byte: Decoded salt bytes
//   - error: nil if decoding succeeds, or an error describing the failure
//
// The function will respond with HTTP 500 Internal Server Error and return an
// empty byte slice if the decoding fails or if response marshaling fails.
//
// Note: The function logs unauthorized access attempts and server errors
// using the application's logging system.
func decodeSalt(salt string, w http.ResponseWriter) ([]byte, error) {
	s, err := hex.DecodeString(salt)
	if err != nil {
		log.Log().Error("decodeSalt",
			"msg", "Problem decoding salt",
			"err", err.Error())

		body := net.MarshalBody(reqres.AdminLoginResponse{
			Err: reqres.ErrServerFault,
		}, w)
		if body == nil {
			return []byte{}, errors.New("failed to marshal response body")
		}

		net.Respond(http.StatusInternalServerError, body, w)
		log.Log().Info("decodeSalt", "msg", "unauthorized")
		return []byte{}, errors.New("failed to decode salt")
	}
	return s, nil
}







-----

// fetchAdminToken retrieves the administrator authentication token from the
// application state.
// If the token is not set, it handles the error by responding with an
// appropriate HTTP status code and error message.
//
// Parameters:
//   - w: HTTP ResponseWriter for sending error responses
//
// Returns:
//   - string: The administrator token if successfully retrieved
//   - error: nil if the token exists, or an error if the token is not set or
//     response marshaling fails
//
// The function will respond with HTTP 500 Internal Server Error and return an
// empty string if the admin token is not set in the application state.
//
// Note: The function logs server errors using the application's logging system.
func fetchAdminToken(w http.ResponseWriter) (string, error) {
	adminToken := state.AdminSigningToken()
	if adminToken == "" {
		log.Log().Error("routeAdminLogin", "msg", "Admin token not set")

		responseBody := net.MarshalBody(reqres.AdminLoginResponse{
			Err: reqres.ErrServerFault}, w)
		if responseBody == nil {
			return "", errors.New("failed to marshal response body")
		}

		net.Respond(http.StatusInternalServerError, responseBody, w)
		log.Log().Info("routeAdminLogin", "msg", "unauthorized")
		return "", errors.New("admin token not set")
	}
	return adminToken, nil
}



POLICY API SPECIFICATION

/v1/acl/policies:
  post:
    description: Create a new access policy
    request:
      body:
        policy_name: string
        spiffe_id_pattern: string  # Supports regex/prefix matching
        path_pattern: string       # Supports glob patterns
        permissions:
          - read
          - list
        metadata:
          created_by: string
          created_at: timestamp
    response:
      policy_id: string
      status: string

  get:
    description: List all policies
    response:
      policies:
        - policy_id: string
          policy_name: string
          spiffe_id_pattern: string
          path_pattern: string
          permissions: [string]
          metadata:
            created_by: string
            created_at: timestamp
            last_modified: timestamp

/v1/acl/policies/{policy_id}:
  get:
    description: Get specific policy details
  delete:
    description: Remove a policy
  put:
    description: Update a policy

# Policy Evaluation API (for internal use)
/v1/acl/check:
  post:
    description: Check if a SPIFFE ID has access to a path
    request:
      spiffe_id: string
      path: string
      action: string  # read/list
    response:
      allowed: boolean
      matching_policies: [string]  # List of policy IDs that granted access

# Example Policy Document
example_policy:
  policy_name: "web-servers-secrets"
  spiffe_id_pattern: "spiffe://example.org/web-server/*"
  path_pattern: "secrets/web/*"
  permissions:
    - read
    - list
  metadata:
    created_by: "admin@example.org"
    created_at: "2024-11-16T10:00:00Z"








// HandleRequest is a generic request handler for unmarshalling JSON requests
func HandleRequest[Req any, Res any](
    requestBody []byte,
    w http.ResponseWriter,
    errorResponse Res,
) *Req {
    var request Req
    if err := net.HandleRequestError(
        w, json.Unmarshal(requestBody, &request),
    ); err != nil {
        log.Log().Error("HandleRequest",
            "msg", "Problem unmarshalling request",
            "err", err.Error())

        responseBody := net.MarshalBody(errorResponse, w)
        if responseBody == nil {
            return nil
        }

        net.Respond(http.StatusBadRequest, responseBody, w)
        return nil
    }
    return &request
}

// Example usage:
func newSecretUndeleteRequest(
    requestBody []byte,
    w http.ResponseWriter,
) *reqres.SecretUndeleteRequest {
    return HandleRequest[reqres.SecretUndeleteRequest, reqres.SecretUndeleteResponse](
        requestBody,
        w,
        reqres.SecretUndeleteResponse{Err: reqres.ErrBadInput},
    )
}


PROPOSAL: Backup Admins
Backup Admin System:
Each admin can have one designated backup admin
The backup admin can reset passwords if needed
Super admins can assign backups for regular admins
Only super admins can assign backups for other super admins
Super admins can create/delete/suspend admins.
regular admins can only reset passwords and view audit logs

Hierarchy:

Super admins can create/delete other admins (including super admins)
Regular admins have no special privileges except for any backup duties assigned to them


Audit Trail:

All actions are logged with timestamps and acting admin
Tracks who created each admin
Logs password resets and backup assignments


Recovery Scenarios:

If a super admin leaves company (or forgets password): their backup can reset it
If a super admin goes rogue: other super admins can delete them
If a regular admin is compromised: super admins or their backup can reset password

This system addresses the following core issus:
No single point of failure
Clear audit trail
Simple recovery mechanisms
Works in a single binary context


//  may be used elsewhere.
func signToken(token, adminToken []byte, metadata state.TokenMetadata) []byte {
	h := hmac.New(sha256.New, adminToken)
	h.Write(token)
	metadataBytes, _ := json.Marshal(metadata)
	h.Write(metadataBytes)
	return h.Sum(nil)
}
		//cid := crypto.Id()
		//
		//validation.EnsureSafe(source)
		//
		//id, err := s.IdFromRequest(r)
		//
		//if err != nil {
		//	log.WarnLn(&cid, "Handler: blocking insecure svid", id, err)
		//
		//	routeFallback.Fallback(cid, r, w)
		//
		//	return
		//}
		//
		//sid := s.IdAsString(r)
//		//

/*
 The most balanced way is to keep the root key locally in a folder that the
 user configures; restrict access to the key by setting proper file and folder
 permissions and ask for password (instead of key) while logging in.

 	 # First time setup
 	 spike encrypt-config --password-prompt
 	 Enter password: ******
 	 # Encrypts the root key using a key derived from the password
 	 # Saves the encrypted key + salt to config

 	 # Regular usage
 	 spike login --password-prompt
 	 Enter password: ******
 	 # Internally:
 	 # 1. Derives encryption key from password + stored salt
 	 # 2. Decrypts the root key
 	 # 3. Uses root key to get session key


*/

/*
	package config

	import (
		"fmt"
		"os"
		"path/filepath"
		"runtime"
		"syscall"

		"gopkg.in/yaml.v3"
	)

	type Config struct {
		RootKey          string `yaml:"root_key,omitempty"`
		EncryptedRootKey string `yaml:"encrypted_root_key,omitempty"`
		Salt             []byte `yaml:"salt,omitempty"`
		IsEncrypted      bool   `yaml:"is_encrypted"`
	}

	type ConfigManager struct {
		configPath string
		config     Config
	}

	// NewConfigManager creates a new configuration manager
	func NewConfigManager(customPath string) (*ConfigManager, error) {
		configPath, err := resolveConfigPath(customPath)
		if err != nil {
			return nil, fmt.Errorf("failed to resolve config path: %w", err)
		}

		cm := &ConfigManager{
			configPath: configPath,
		}

		// Load existing config if it exists
		if err := cm.loadConfig(); err != nil && !os.IsNotExist(err) {
			return nil, fmt.Errorf("failed to load config: %w", err)
		}

		return cm, nil
	}

	// resolveConfigPath determines the configuration file path
	func resolveConfigPath(customPath string) (string, error) {
		if customPath != "" {
			absPath, err := filepath.Abs(customPath)
			if err != nil {
				return "", err
			}
			return absPath, nil
		}

		// Get user's home directory
		home, err := os.UserHomeDir()
		if err != nil {
			return "", fmt.Errorf("failed to get home directory: %w", err)
		}

		// Default path is ~/.spike/config.yaml
		return filepath.Join(home, ".spike", "config.yaml"), nil
	}

	// ensureConfigDir ensures the configuration directory exists with proper permissions
	func (cm *ConfigManager) ensureConfigDir() error {
		configDir := filepath.Dir(cm.configPath)

		// Create directory with restricted permissions
		err := os.MkdirAll(configDir, 0700)
		if err != nil {
			return fmt.Errorf("failed to create config directory: %w", err)
		}

		// On Unix-like systems, explicitly set directory permissions
		if runtime.GOOS != "windows" {
			if err := os.Chmod(configDir, 0700); err != nil {
				return fmt.Errorf("failed to set directory permissions: %w", err)
			}
		}

		return nil
	}

	// loadConfig loads the configuration from file
	func (cm *ConfigManager) loadConfig() error {
		data, err := os.ReadFile(cm.configPath)
		if err != nil {
			return err
		}

		return yaml.Unmarshal(data, &cm.config)
	}

	// saveConfig saves the configuration to file with proper permissions
	func (cm *ConfigManager) saveConfig() error {
		if err := cm.ensureConfigDir(); err != nil {
			return err
		}

		// Create or truncate the config file with restricted permissions
		file, err := os.OpenFile(cm.configPath, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)
		if err != nil {
			return fmt.Errorf("failed to open config file: %w", err)
		}
		defer file.Close()

		// On Unix-like systems, explicitly set file permissions
		if runtime.GOOS != "windows" {
			if err := file.Chmod(0600); err != nil {
				return fmt.Errorf("failed to set file permissions: %w", err)
			}
		}

		data, err := yaml.Marshal(cm.config)
		if err != nil {
			return fmt.Errorf("failed to marshal config: %w", err)
		}

		if _, err := file.Write(data); err != nil {
			return fmt.Errorf("failed to write config: %w", err)
		}

		return nil
	}

	// SetRootKey saves the root key in plain text
	func (cm *ConfigManager) SetRootKey(rootKey string) error {
		cm.config.RootKey = rootKey
		cm.config.IsEncrypted = false
		// Clear any encrypted data
		cm.config.EncryptedRootKey = ""
		cm.config.Salt = nil

		return cm.saveConfig()
	}

	// GetRootKey retrieves the root key
	func (cm *ConfigManager) GetRootKey() (string, error) {
		if cm.config.IsEncrypted {
			// Handle encrypted case
			password, err := promptPassword("Enter password to decrypt config: ")
			if err != nil {
				return "", fmt.Errorf("failed to read password: %w", err)
			}

			key := deriveKey(password, cm.config.Salt)
			encrypted, err := base64.StdEncoding.DecodeString(cm.config.EncryptedRootKey)
			if err != nil {
				return "", fmt.Errorf("failed to decode encrypted data: %w", err)
			}

			return decrypt(key, encrypted)
		}

		return cm.config.RootKey, nil
	}

	// EnableEncryption converts plain text storage to encrypted storage
	func (cm *ConfigManager) EnableEncryption() error {
		if cm.config.IsEncrypted {
			return fmt.Errorf("encryption is already enabled")
		}

		if cm.config.RootKey == "" {
			return fmt.Errorf("no root key to encrypt")
		}

		password, err := promptPassword("Enter password to encrypt config: ")
		if err != nil {
			return fmt.Errorf("failed to read password: %w", err)
		}

		// Generate a random salt
		salt := make([]byte, saltLength)
		if _, err := rand.Read(salt); err != nil {
			return fmt.Errorf("failed to generate salt: %w", err)
		}

		// Derive encryption key and encrypt
		key := deriveKey(password, salt)
		encrypted, err := encrypt(key, cm.config.RootKey)
		if err != nil {
			return fmt.Errorf("encryption failed: %w", err)
		}

		// Update config with encrypted data
		cm.config.EncryptedRootKey = base64.StdEncoding.EncodeToString(encrypted)
		cm.config.Salt = salt
		cm.config.IsEncrypted = true
		cm.config.RootKey = "" // Clear plain text key

		return cm.saveConfig()
	}

	// DisableEncryption converts encrypted storage to plain text
	func (cm *ConfigManager) DisableEncryption() error {
		if !cm.config.IsEncrypted {
			return fmt.Errorf("encryption is not enabled")
		}

		// Get the decrypted key first
		rootKey, err := cm.GetRootKey()
		if err != nil {
			return fmt.Errorf("failed to decrypt key: %w", err)
		}

		// Clear encrypted data and store as plain text
		cm.config.EncryptedRootKey = ""
		cm.config.Salt = nil
		cm.config.IsEncrypted = false
		cm.config.RootKey = rootKey

		return cm.saveConfig()
	}

*/

/*
	package config

	import (
		"crypto/aes"
		"crypto/cipher"
		"crypto/rand"
		"encoding/base64"
		"encoding/json"
		"fmt"
		"golang.org/x/crypto/argon2"
		"io"
		"os"
		"syscall"
		"golang.org/x/term"
	)

	// SecureConfig holds our configuration data
	type SecureConfig struct {
		EncryptedRootKey string `json:"encrypted_root_key"`
		Salt            []byte `json:"salt"`
		// Add other fields as needed
	}

	// Parameters for Argon2 key derivation
	const (
		keyLength  = 32 // for AES-256
		saltLength = 16
		time       = 1
		memory     = 64 * 1024
		threads    = 4
	)

	// promptPassword securely prompts for password
	func promptPassword(prompt string) (string, error) {
		fmt.Print(prompt)
		password, err := term.ReadPassword(int(syscall.Stdin))
		fmt.Println() // Add newline after input
		if err != nil {
			return "", err
		}
		return string(password), nil
	}

	// deriveKey generates an encryption key from a password using Argon2
	func deriveKey(password string, salt []byte) []byte {
		return argon2.IDKey([]byte(password), salt, time, memory, threads, keyLength)
	}

	// encrypt encrypts data using AES-GCM
	func encrypt(key []byte, plaintext string) ([]byte, error) {
		block, err := aes.NewCipher(key)
		if err != nil {
			return nil, err
		}

		gcm, err := cipher.NewGCM(block)
		if err != nil {
			return nil, err
		}

		nonce := make([]byte, gcm.NonceSize())
		if _, err := io.ReadFull(rand.Reader, nonce); err != nil {
			return nil, err
		}

		ciphertext := gcm.Seal(nonce, nonce, []byte(plaintext), nil)
		return ciphertext, nil
	}

	// decrypt decrypts data using AES-GCM
	func decrypt(key []byte, ciphertext []byte) (string, error) {
		block, err := aes.NewCipher(key)
		if err != nil {
			return "", err
		}

		gcm, err := cipher.NewGCM(block)
		if err != nil {
			return "", err
		}

		if len(ciphertext) < gcm.NonceSize() {
			return "", fmt.Errorf("ciphertext too short")
		}

		nonce, ciphertext := ciphertext[:gcm.NonceSize()], ciphertext[gcm.NonceSize():]
		plaintext, err := gcm.Open(nil, nonce, ciphertext, nil)
		if err != nil {
			return "", err
		}

		return string(plaintext), nil
	}

	// EncryptConfig encrypts the root key with a password
	func EncryptConfig(rootKey string) error {
		password, err := promptPassword("Enter password to encrypt config: ")
		if err != nil {
			return fmt.Errorf("failed to read password: %v", err)
		}

		// Generate a random salt
		salt := make([]byte, saltLength)
		if _, err := rand.Read(salt); err != nil {
			return fmt.Errorf("failed to generate salt: %v", err)
		}

		// Derive encryption key from password
		key := deriveKey(password, salt)

		// Encrypt the root key
		encrypted, err := encrypt(key, rootKey)
		if err != nil {
			return fmt.Errorf("encryption failed: %v", err)
		}

		config := SecureConfig{
			EncryptedRootKey: base64.StdEncoding.EncodeToString(encrypted),
			Salt:            salt,
		}

		// Save to file
		file, err := os.OpenFile(".spike-config", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)
		if err != nil {
			return fmt.Errorf("failed to open config file: %v", err)
		}
		defer file.Close()

		return json.NewEncoder(file).Encode(config)
	}

	// DecryptConfig decrypts and returns the root key
	func DecryptConfig() (string, error) {
		// Read config file
		file, err := os.Open(".spike-config")
		if err != nil {
			return "", fmt.Errorf("failed to open config file: %v", err)
		}
		defer file.Close()

		var config SecureConfig
		if err := json.NewDecoder(file).Decode(&config); err != nil {
			return "", fmt.Errorf("failed to decode config: %v", err)
		}

		// Get password from user
		password, err := promptPassword("Enter password to decrypt config: ")
		if err != nil {
			return "", fmt.Errorf("failed to read password: %v", err)
		}

		// Derive key from password and salt
		key := deriveKey(password, config.Salt)

		// Decode base64 encrypted data
		encrypted, err := base64.StdEncoding.DecodeString(config.EncryptedRootKey)
		if err != nil {
			return "", fmt.Errorf("failed to decode encrypted data: %v", err)
		}

		// Decrypt the root key
		rootKey, err := decrypt(key, encrypted)
		if err != nil {
			return "", fmt.Errorf("decryption failed: %v", err)
		}

		return rootKey, nil
	}

*/

/*
Create encrypted backup of the root key.

type KeyBackup struct {
    Version          int       `json:"version"`
    EncryptedKey     string    `json:"encrypted_key"`
    KeyID            string    `json:"key_id"`
    Timestamp        time.Time `json:"timestamp"`
    EncryptionParams struct {
        Algorithm string `json:"algorithm"`
        KDF      string `json:"kdf"`
        Salt     []byte `json:"salt"`
    } `json:"encryption_params"`
}

func createBackup(rootKey []byte, recoveryPassword string) (*KeyBackup, error) {
    // Generate strong encryption parameters
    salt := make([]byte, 32)
    rand.Read(salt)

    // Derive key from recovery password
    key := deriveKey(recoveryPassword, salt)

    // Encrypt root key
    encrypted, err := encrypt(key, rootKey)
    if err != nil {
        return nil, err
    }

    return &KeyBackup{
        Version:      1,
        EncryptedKey: base64.StdEncoding.EncodeToString(encrypted),
        KeyID:        generateKeyID(rootKey),
        Timestamp:    time.Now(),
        EncryptionParams: struct {
            Algorithm string `json:"algorithm"`
            KDF      string `json:"kdf"`
            Salt     []byte `json:"salt"`
        }{
            Algorithm: "AES-256-GCM",
            KDF:      "Argon2id",
            Salt:     salt,
        },
    }, nil
}


*/

/*
func recoverRootKey(backup *KeyBackup, recoveryPassword string) ([]byte, error) {
    // Derive key from recovery password using stored params
    key := deriveKey(recoveryPassword, backup.EncryptionParams.Salt)

    // Decode and decrypt
    encrypted, err := base64.StdEncoding.DecodeString(backup.EncryptedKey)
    if err != nil {
        return nil, err
    }

    rootKey, err := decrypt(key, encrypted)
    if err != nil {
        return nil, err
    }

    // Verify key ID matches
    if generateKeyID(rootKey) != backup.KeyID {
        return nil, errors.New("key verification failed")
    }

    return rootKey, nil
}
*/

/*
Use a strong recovery password (high entropy)
Consider splitting recovery password using Shamir's Secret Sharing
Regular testing of recovery process
Maintain audit log of backup access
Version control for backup format
Backup rotation strategy

*/
-----
Issue management:
* This is a tiny project; so it does not need a big fat issue manager.
  even a `to_do.txt` with every line in priority order is a good enough way
  to manage things.
* The development team (me, Volkan, initially) will use `to do` labels liberally
  to designate what to do where in the project.
* GitHub issues will be created on a "per need" basis.
* Also the community will be encouraged to create GitHub issues, yet it won't
  be the team's main way to define issues or roadmap.
* I believe this unorthodox way will provide agility.
* For documentation versions, redirect to tagged github snapshots.
======

package main

import (
	"crypto/aes"
	"crypto/cipher"
	"crypto/rand"
	"database/sql"
	"encoding/json"
	"fmt"
	"io"
	"time"

	_ "github.com/lib/pq"
)

// SecretRecord represents how we'll store the secret in postgres
type SecretRecord struct {
	Path       string    `json:"path"`
	Data       []byte    `json:"data"`       // encrypted Version map
	Metadata   Metadata  `json:"metadata"`   // stored in plain text
	UpdatedAt  time.Time `json:"updated_at"`
	CreatedAt  time.Time `json:"created_at"`
}

// encrypt uses AES-GCM to encrypt the data with the given key
func encrypt(key []byte, plaintext []byte) ([]byte, error) {
	block, err := aes.NewCipher(key)
	if err != nil {
		return nil, fmt.Errorf("creating cipher: %w", err)
	}

	gcm, err := cipher.NewGCM(block)
	if err != nil {
		return nil, fmt.Errorf("creating GCM: %w", err)
	}

	nonce := make([]byte, gcm.NonceSize())
	if _, err := io.ReadFull(rand.Reader, nonce); err != nil {
		return nil, fmt.Errorf("creating nonce: %w", err)
	}

	// Encrypt and prepend nonce
	ciphertext := gcm.Seal(nonce, nonce, plaintext, nil)
	return ciphertext, nil
}

// UpsertSecret encrypts and stores the secret in Postgres
func UpsertSecret(path string, values map[string]string) error {
	kvMu.Lock()
	defer kvMu.Unlock()

	// First, update the in-memory KV store
	kv.Put(path, values)

	// Get the secret we just stored
	secret, exists := kv.data[path]
	if !exists {
		return fmt.Errorf("failed to retrieve secret after Put")
	}

	// Serialize the versions map to JSON
	versionsJSON, err := json.Marshal(secret.Versions)
	if err != nil {
		return fmt.Errorf("marshaling versions: %w", err)
	}

	// Encrypt the versions data
	encryptedData, err := encrypt([]byte(RootKey()), versionsJSON)
	if err != nil {
		return fmt.Errorf("encrypting data: %w", err)
	}

	// Connect to postgres
	db, err := sql.Open("postgres", "postgresql://localhost:5432/spike?sslmode=disable")
	if err != nil {
		return fmt.Errorf("connecting to database: %w", err)
	}
	defer db.Close()

	// Upsert query using ON CONFLICT
	query := `
		INSERT INTO secrets (
			path,
			encrypted_data,
			metadata,
			updated_at,
			created_at
		) VALUES ($1, $2, $3, $4, $4)
		ON CONFLICT (path) DO UPDATE SET
			encrypted_data = EXCLUDED.encrypted_data,
			metadata = EXCLUDED.metadata,
			updated_at = EXCLUDED.updated_at
	`

	metadataJSON, err := json.Marshal(secret.Metadata)
	if err != nil {
		return fmt.Errorf("marshaling metadata: %w", err)
	}

	now := time.Now()

	_, err = db.Exec(query,
		path,
		encryptedData,
		metadataJSON,
		now,
	)
	if err != nil {
		return fmt.Errorf("upserting to database: %w", err)
	}

	return nil
}

// SQL to create the table:
const createTableSQL = `
CREATE TABLE IF NOT EXISTS secrets (
    path TEXT PRIMARY KEY,
    encrypted_data BYTEA NOT NULL,
    metadata JSONB NOT NULL,
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL
);
`
----

ADR:

It's generally considered better security practice to create the schema out-of-band (separate from the application) for several reasons:

Principle of Least Privilege:

The application should only have the permissions it needs for runtime (INSERT, UPDATE, SELECT, etc.)
Schema modification rights (CREATE TABLE, ALTER TABLE, etc.) are not needed during normal operation
This limits potential damage if the application is compromised


Change Management:

Database schema changes can be managed through proper migration tools
Changes can be reviewed, versioned, and rolled back if needed
Prevents accidental schema modifications during application restarts


Environment Consistency:

Ensures all environments (dev, staging, prod) have identical schemas
Reduces risk of schema drift between environments
Makes it easier to track schema changes in version control


/*
func parseVersionFlag(arg string) (int, error) {
    var version int
    if strings.HasPrefix(arg, "-version=") {
        _, err := fmt.Sscanf(arg, "-version=%d", &version)
        return version, err
    }
    return 0, fmt.Errorf("invalid version flag: %s", arg)
}

func parseVersionsFlag(arg string) ([]int, error) {
    if !strings.HasPrefix(arg, "-versions=") {
        return nil, fmt.Errorf("invalid versions flag: %s", arg)
    }
    versions := strings.Split(strings.TrimPrefix(arg, "-versions="), ",")
    result := make([]int, 0, len(versions))
    for _, v := range versions {
        n, err := strconv.Atoi(v)
        if err != nil {
            return nil, fmt.Errorf("invalid version number: %s", v)
        }
        result = append(result, n)
    }
    return result, nil
}
//func doPost(client *http.Client, path string, mr []byte) error {
//	r, err := client.Post(path, "application/json", bytes.NewBuffer(mr))
//
//	if err != nil {
//		return errors.Join(
//			err,
//			errors.New("post: Problem connecting to SPIKE Nexus API endpoint URL"),
//		)
//	}
//
//	if r.StatusCode != http.StatusOK {
//		return errors.New("post: Problem connecting to SPIKE Nexus API endpoint URL")
//	}
//
//	respond(r)
//	return nil
//}
*/

//func doPost(client *http.Client, p string, md []byte) error {
//	r, err := client.Post(p, "application/json", bytes.NewBuffer(md))
//
//	if err != nil {
//		return errors.Join(
//			err,
//			errors.New("post: Problem connecting to SPIKE Keep:"+err.Error()),
//		)
//	}
//
//	if r.StatusCode != http.StatusOK {
//		return errors.New("post: Problem connecting SPIKE Keep: status:" + r.Status)
//	}
//
//	respond(r)
//
//	return nil
//}

### SPIKE Mint

**SPIKE Mint** is a standalone utility for generating initialization tokens.
It creates cryptographically secure tokens with metadata:

**Features**:
* 256 bits of entropy per token
* Version tracking
* Creation timestamp
* Purpose field
* Optional expiry
* HMAC-based integrity verification
* Base64 encoded for transport
* CLI interface for admin usage
* Can validate existing tokens
* Ensures standardization of token format and security

## Future Considerations

### Potential Enhancements

* Automated key rotation
* Multiple Keeper instances for redundancy
* Various backing stores (file system, postgres, cloud secrets store)
* Key versioning for tracking encryption history

### Additional Monitoring

* Advanced authentication metrics
* Key usage statistics
* Performance metrics for crypto operations.

## Security Recommendations

* Use filesystem encryption
* Regular security audits for machines and key components
* Strict access control
* Regular backup verification of encrpted root key
* Monitoring for anomalies and unusual access patterns


package handle

import "github.com/spiffe/go-spiffe/v2/workloadapi"

//// parseVersions helper function to parse version numbers from command args
//func parseVersions(args []string) []int {
//	versions := []int{}
//	for _, arg := range args {
//		if strings.HasPrefix(arg, "-versions=") {
//			versionStr := strings.TrimPrefix(arg, "-versions=")
//			for _, v := range strings.Split(versionStr, ",") {
//				var version int
//				fmt.Sscanf(v, "%d", &version)
//				versions = append(versions, version)
//			}
//			break
//		}
//	}
//	return versions
//}

func Delete(source *workloadapi.X509Source, args []string) {
panic("handleDelete not implemented")

//		if len(args) < 3 {
//			fmt.Println("Usage: pilot delete <path> [-versions=<n1,n2,...>]")
//			return
//		}
//		versions := parseVersions(args)
//		if err := store.softDelete(args[2], versions); err != nil {
//			fmt.Printf("Error: %v\n", err)
//			return
//		}
//		fmt.Printf("Success! Versions marked as deleted at: %s\n", args[2])
}
